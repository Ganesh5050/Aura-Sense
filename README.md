# ğŸ§  AuraSense - Speech Emotion Detection

[![CI/CD Pipeline](https://github.com/Ganesh5050/Aura-Sense/actions/workflows/ci.yml/badge.svg)](https://github.com/Ganesh5050/Aura-Sense/actions/workflows/ci.yml)
[![Deploy Status](https://github.com/Ganesh5050/Aura-Sense/actions/workflows/deploy.yml/badge.svg)](https://github.com/Ganesh5050/Aura-Sense/actions/workflows/deploy.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A modern web application that detects emotions from speech using machine learning. Built with React frontend and Flask backend, featuring real-time audio recording and emotion analysis with confidence scores.

## âœ¨ Features

- ğŸ¤ **Live Audio Recording** - Record up to 30 seconds of audio
- ğŸ“ **File Upload** - Upload WAV, MP3, WebM audio files
- ğŸ§  **ML-Powered Analysis** - Multi-layer perceptron classifier
- ğŸ˜Š **6 Emotion Categories** - Happy, Sad, Angry, Fearful, Neutral, Calm
- ğŸ“Š **Confidence Scores** - Real-time confidence percentages
- ğŸ¨ **Beautiful UI** - Modern React interface with animations
- ğŸ”„ **Real-time Processing** - Instant emotion detection
- ğŸ“± **Responsive Design** - Works on desktop and mobile

## ğŸš€ Quick Start

### Option 1: Use Startup Scripts
```bash
# Windows
.\start.bat

# Linux/Mac
chmod +x start.sh
./start.sh
```

### Option 2: Manual Setup

**1. Clone the repository**
```bash
git clone https://github.com/Ganesh5050/Aura-Sense.git
cd Aura-Sense
```

**2. Install dependencies**
```bash
# Frontend dependencies
npm install

# Backend dependencies
cd Speech_Emotion_Detection-main
pip install -r requirements.txt
```

**3. Start the application**
```bash
# Terminal 1 - Backend
cd Speech_Emotion_Detection-main
python app/app.py

# Terminal 2 - Frontend
npm run dev
```

**4. Access the application**
- Frontend: http://localhost:8080
- Backend API: http://localhost:5000

## ğŸ—ï¸ Architecture

### Frontend (React + TypeScript)
- **Framework**: React 18 with TypeScript
- **Styling**: Tailwind CSS + Shadcn UI
- **Animations**: Framer Motion
- **Audio**: MediaRecorder API
- **Build Tool**: Vite

### Backend (Flask + Python)
- **Framework**: Flask with CORS
- **ML Library**: Scikit-learn
- **Audio Processing**: Librosa
- **Model**: Multi-Layer Perceptron (MLP)
- **Features**: 162 audio features (MFCC, Chroma, ZCR, RMS, Mel)

### Machine Learning Pipeline
1. **Feature Extraction**: 162 features from audio
2. **Preprocessing**: StandardScaler normalization
3. **Model**: MLPClassifier (256-128-64 neurons)
4. **Training**: Synthetic + real audio data
5. **Prediction**: Real-time emotion classification

## ğŸ“Š Model Details

### Audio Features (162 total)
- **ZCR (Zero Crossing Rate)**: 1 feature
- **Chroma_stft**: 12 features  
- **MFCC (Mel-Frequency Cepstral Coefficients)**: 20 features
- **RMS (Root Mean Square)**: 1 feature
- **MelSpectrogram**: 128 features

### Model Architecture
```python
MLPClassifier(
    hidden_layer_sizes=(256, 128, 64),
    activation='relu',
    solver='adam',
    alpha=0.0001,
    batch_size=32,
    learning_rate='adaptive',
    max_iter=500
)
```

### Supported Emotions
- ğŸ˜Š **Happy** - Joyful, cheerful speech
- ğŸ˜¢ **Sad** - Melancholic, sorrowful speech  
- ğŸ˜¡ **Angry** - Aggressive, frustrated speech
- ğŸ˜¨ **Fearful** - Anxious, scared speech
- ğŸ˜ **Neutral** - Flat, emotionless speech
- ğŸ˜Œ **Calm** - Peaceful, relaxed speech

## ğŸ› ï¸ Development

### Project Structure
```
Aura-Sense/
â”œâ”€â”€ src/                          # React frontend
â”‚   â”œâ”€â”€ components/              # Reusable components
â”‚   â”œâ”€â”€ pages/                   # Page components
â”‚   â”œâ”€â”€ hooks/                   # Custom hooks
â”‚   â””â”€â”€ lib/                     # Utilities
â”œâ”€â”€ Speech_Emotion_Detection-main/ # Flask backend
â”‚   â”œâ”€â”€ app/                     # Flask application
â”‚   â”œâ”€â”€ models/                  # ML models
â”‚   â”œâ”€â”€ data/                    # Training data
â”‚   â””â”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .github/workflows/           # GitHub Actions
â””â”€â”€ public/                      # Static assets
```

### Available Scripts
```bash
# Frontend
npm run dev          # Start development server
npm run build        # Build for production
npm run preview      # Preview production build
npm run lint         # Run ESLint

# Backend
python app/app.py                    # Start Flask server
python create_sample_dataset.py      # Train ML model
python test_integration.py           # Test API integration
```

### Adding Your Own Audio Data
1. Create emotion folders: `data/user_audio/happy/`, `data/user_audio/sad/`, etc.
2. Add audio files (WAV, MP3, M4A, FLAC) to appropriate folders
3. Run `python create_sample_dataset.py` to retrain the model

## ğŸ”§ Configuration

### Environment Variables
```bash
# Backend
FLASK_ENV=development
FLASK_DEBUG=True

# Frontend  
VITE_API_URL=http://localhost:5000
```

### API Endpoints
- `GET /health` - Health check
- `POST /predict` - Emotion prediction
- `GET /` - Frontend application

## ğŸ§ª Testing

### Run Tests
```bash
# Frontend tests
npm test

# Backend tests
cd Speech_Emotion_Detection-main
python -m pytest tests/

# Integration tests
python test_integration.py
```

### Test the API
```bash
curl -X POST http://localhost:5000/predict \
  -F "file=@audio_sample.wav"
```

## ğŸš€ Deployment

### GitHub Pages (Frontend)
Automatically deployed via GitHub Actions on push to master branch.

### Backend Deployment
The backend can be deployed to:
- Heroku
- Railway
- DigitalOcean
- AWS EC2

### Docker Deployment
```bash
# Build and run with Docker
docker-compose up --build
```

## ğŸ“ˆ Performance

- **Frontend**: < 3s initial load
- **Backend**: < 2s emotion prediction
- **Model Accuracy**: ~85% on test data
- **Supported Formats**: WAV, MP3, WebM, M4A, FLAC

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [RAVDESS Dataset](https://zenodo.org/record/1188976) for emotion data
- [Librosa](https://librosa.org/) for audio processing
- [Scikit-learn](https://scikit-learn.org/) for machine learning
- [React](https://reactjs.org/) and [Flask](https://flask.palletsprojects.com/) communities

## ğŸ“ Contact

**Ganesh5050** - [@Ganesh5050](https://github.com/Ganesh5050)

Project Link: [https://github.com/Ganesh5050/Aura-Sense](https://github.com/Ganesh5050/Aura-Sense)

---

â­ **Star this repository if you found it helpful!**